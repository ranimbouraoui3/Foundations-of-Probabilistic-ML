{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "611aac3d",
      "metadata": {
        "id": "611aac3d"
      },
      "source": [
        "# Training a Fashion Image Classifier with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup"
      ],
      "metadata": {
        "id": "egIhWQ-Jb1wM"
      },
      "id": "egIhWQ-Jb1wM"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ea99cdc9",
      "metadata": {
        "id": "ea99cdc9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the Dataset\n",
        "We use the torchvision.datasets API to pull the FashionMNIST data. This dataset consists of 70,000 grayscale images of clothing across 10 categories."
      ],
      "metadata": {
        "id": "PZ9ErHV5b-5j"
      },
      "id": "PZ9ErHV5b-5j"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f6666aef",
      "metadata": {
        "id": "f6666aef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14cd854b-d4ec-4b46-d7fe-8265889b2d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 9.66MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 149kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 2.75MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 14.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the DataLoaders"
      ],
      "metadata": {
        "id": "lTcGRIJ9cOoc"
      },
      "id": "lTcGRIJ9cOoc"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "581aa816",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "581aa816",
        "outputId": "0f379d81-4bef-43c0-fa6f-7a0fee955064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e26be20c",
      "metadata": {
        "id": "e26be20c"
      },
      "source": [
        "### Basic Tensor Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "35e48fdd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35e48fdd",
        "outputId": "89cdb8e2-a0b5-456e-c005-4ca40fe1fea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor :\n",
            " tensor([[0.5672, 0.8801, 0.0197],\n",
            "        [0.9543, 0.9752, 0.1437]])\n",
            "Tensor Shape: torch.Size([2, 3])\n",
            "Tensor Type: torch.float32\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create a 2x3 tensor with random numbers\n",
        "tensor = torch.rand(2, 3)  # values sampled between 0 and 1\n",
        "\n",
        "# Displaying the tensor\n",
        "print(\"Tensor :\\n\", tensor)\n",
        "\n",
        "# Checking the shape (dimensions)\n",
        "print(\"Tensor Shape:\", tensor.size())  # Alternatively: tensor.shape\n",
        "\n",
        "# Checking the data type\n",
        "print(\"Tensor Type:\", tensor.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ecc1d572",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecc1d572",
        "outputId": "3e78b0e2-a9ab-43f6-cf65-729dfd77bfaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix Product :\n",
            " tensor([[19, 22],\n",
            "        [43, 50]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Creating two 2x2 tensors manually\n",
        "tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
        "tensor2 = torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "# Perform Matrix Multiplication\n",
        "matmul_result = torch.matmul(tensor1, tensor2)\n",
        "\n",
        "print(\"Matrix Product :\\n\", matmul_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f9b443b",
      "metadata": {
        "id": "1f9b443b"
      },
      "source": [
        "## Implementing a Custom Neural Network Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "43b0d579",
      "metadata": {
        "id": "43b0d579"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the model architecture\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(1, 1)    # 1 input feature, 1 output neuron\n",
        ")\n",
        "\n",
        "# Define Loss Function and Optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8bbec70",
      "metadata": {
        "id": "c8bbec70"
      },
      "source": [
        "**DESIGN NOTE:**  \n",
        "This implementation exploits PyTorch's imperative execution.  \n",
        "\n",
        "- `__init__`: Encapsulates the structural state and learnable parameters.  \n",
        "- `forward`: Defines the dynamic transformation logic.  \n",
        "\n",
        "The define-by-run nature of this framework is critical for the **DynamicCNN** modality-switching logic, enabling flexible data flow that is essential for complex research architectures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1b950954",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b950954",
        "outputId": "48850ce6-5b5d-4451-8d56-047f7cf7ebd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleModel(\n",
            "  (fc): Linear(in_features=5, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Structural State: Define the learnable parameters\n",
        "        self.fc = nn.Linear(5, 10)   # Dense layer: 5 inputs -> 10 outputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Transformation Logic: Define the flow of data\n",
        "        x = F.relu(self.fc(x))       # Linear layer followed by ReLU activation\n",
        "        return x\n",
        "\n",
        "# Instantiating the model\n",
        "model = SimpleModel()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a55798c",
      "metadata": {
        "id": "5a55798c"
      },
      "source": [
        "### Defining the CNN Architecture for FashionMNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "52d7bdc4",
      "metadata": {
        "id": "52d7bdc4"
      },
      "outputs": [],
      "source": [
        "class CNNFashionMNIST(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 1. Extraction de caractéristiques : Convolution\n",
        "        # Entrée : (1, 28, 28) -> Sortie : (32, 26, 26)\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "\n",
        "        # 2. Réduction de dimension : Max Pooling\n",
        "        # Entrée : (32, 26, 26) -> Sortie : (32, 13, 13)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # 3. Classification : Couches entièrement connectées (Dense)\n",
        "        # Aplatissement : 32 canaux * 13 de large * 13 de haut = 5408 caractéristiques\n",
        "        self.fc1 = nn.Linear(32 * 13 * 13, 128)\n",
        "        self.fc2 = nn.Linear(128, 10) # 10 classes pour FashionMNIST\n",
        "\n",
        "# Initialisation\n",
        "model = CNNFashionMNIST()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55d0bed0",
      "metadata": {
        "id": "55d0bed0"
      },
      "source": [
        "## Calculating Total Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3fa6efd1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fa6efd1",
        "outputId": "349a8508-ff0d-48ea-f66d-a4f7c9edd9e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNFashionMNIST(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=5408, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep CNN Architecture for FashionMNIST"
      ],
      "metadata": {
        "id": "AwN1t3nfeM2u"
      },
      "id": "AwN1t3nfeM2u"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "705e1f2f",
      "metadata": {
        "id": "705e1f2f"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNFashionMNIST(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Block 1: Extracts low-level features (edges)\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Block 2: Extracts mid-level features (shapes)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Classifier Head\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x)) # Output: (32, 26, 26)\n",
        "        x = self.pool1(x)         # Output: (32, 13, 13)\n",
        "\n",
        "        x = F.relu(self.conv2(x)) # Output: (64, 11, 11)\n",
        "        x = self.pool2(x)         # Output: (64, 5, 5)\n",
        "\n",
        "        # Correct Flattening for batches\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x) # Logits output for CrossEntropyLoss\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "551782fb",
      "metadata": {
        "id": "551782fb"
      },
      "source": [
        "## Dynamic Modality Switching: Grayscale vs. RGB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "592b55d7",
      "metadata": {
        "id": "592b55d7"
      },
      "outputs": [],
      "source": [
        "class DynamicCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Convolution + ReLU\n",
        "        self.convNG = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)  # (28x28x1) image NG-> (26x26x32) dimension feature map\n",
        "        self.convRGB = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)                      # MaxPooling (2x2) -> (13x13x32), stride = padding\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # (11x11x64) -> (5x5x64)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fcNG = nn.Linear(32 * 13 * 13, 128)\n",
        "        self.fcRGB = nn.Linear(64 * 5 * 5, 128)  # Dense cachée 32: feature map, 13 * 13 => neurones\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "    def forward(self,x):\n",
        "        if(x.shape[1] ==1):\n",
        "            x = F.relu(self.convNG(x))\n",
        "            x = self.pool1(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = F.relu(self.fcNG(x))\n",
        "            x = self.fc2(x)\n",
        "            return x\n",
        "        else:\n",
        "            x = F.relu(self.convRGB(x))\n",
        "            x = self.pool1(x)\n",
        "            x = F.relu(self.conv2(x))\n",
        "            x= self.pool2(x)\n",
        "            x = x.view(x.size(0), -1) #Flatten  0: garder la dimension du batch, -1: flatten all dimensions to one\n",
        "            x = F.relu(self.fcRGB(x))\n",
        "            x = self.fc2(x) # Don't apply relu because this is the classification layer (tête de classification), the layer in which we will determine which class our input belong to.\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real-Time Modality Testing"
      ],
      "metadata": {
        "id": "k6F-CCQveZb8"
      },
      "id": "k6F-CCQveZb8"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "355237d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "355237d1",
        "outputId": "f83789b9-e9b8-4c0a-bdbc-f9c9f7231d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grayscale output shape: torch.Size([2, 10])\n",
            "RGB input shape: torch.Size([2, 3, 28, 28])\n",
            "RGB output shape: torch.Size([2, 10])\n"
          ]
        }
      ],
      "source": [
        "model = DynamicCNN()\n",
        "\n",
        "# Test with grayscale image (batch of 1, 1 channel, 28x28)\n",
        "gray_img = torch.randn(2, 1, 28, 28)  # Batch size 2\n",
        "out_gray = model(gray_img)\n",
        "print(\"Grayscale output shape:\", out_gray.shape)  # Should be (2, 10)\n",
        "\n",
        "# Test with RGB image (batch of 2, 3 channels, 28x28)\n",
        "rgb_img = torch.randn(2, 3, 28, 28)  # Batch size 2\n",
        "print(\"RGB input shape:\", rgb_img.shape)\n",
        "out_rgb = model(rgb_img)\n",
        "print(\"RGB output shape:\", out_rgb.shape)  # Should be (2, 10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch-tp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}